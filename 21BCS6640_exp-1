import os 
import nltk
import nltk.corpus
nltk.download('gutenberg')

nltk.download('punkt')

text = input("Enter your text: ")

word_tokens = word_tokenize(text)
print("Word Tokenization:")
print(word_tokens)

sent_tokens = sent_tokenize(text)
print("\nSentence Tokenization:")
print(sent_tokens)

regexp_tokens = regexp_tokenize(text, pattern=r'\w+|\$[\d\.]+|\S+')
print("\nRegExp Tokenization:")
print(regexp_tokens)

tweet_tokenizer = TweetTokenizer()
tweet_tokens = tweet_tokenizer.tokenize(text)
print("\nTweet Tokenization:")
print(tweet_tokens)
